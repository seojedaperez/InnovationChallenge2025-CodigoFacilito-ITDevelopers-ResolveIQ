00:00 Hello, and welcome to the November Innovation Challenge Hackathon. If you noticed right there at the beginning, I messed up on the theme music one more time. I do that when I'm the only person presenting. Unfortunately, my guest today will not be able to join us, so we're going to improvise a little bit. 
00:43 My guest today will not be able to join us, so we're going to improvise a little bit. I was going to have a fellow come talk to us who was a winner of an internal Microsoft Hackathon. I had a bunch of questions for him about here's what his advice would be on a hackathon, but he wasn't able to make it. 
01:00 Instead, what we're going to do, we're going to start off a little audience participation. Get ready to put some things into chat as I ask questions. Then I'm going to take a little bit of time to actually go through some of the challenges that we've put out for the hackathon, and maybe give some tips and advice, and we'll use Microsoft Co-Pilot to give us suggestions on them. 
01:24 I thought that would be helpful for everyone. But I am super excited that everyone is here. There's a lot of excitement around the hackathon. I see it in my messages, I see it on LinkedIn. There's a lot of excitement internally at Microsoft too. I've got 50 people volunteering to be judges and advisors. And so it's, you know, take the time to really learn something new.
01:53 You're going to have some skills in two weeks that you don't have right now. And I think, again, the key thing for this hackathon is there's not a lot of people who've built really solid enterprise quality AI solutions. Yet, at the end of this hackathon, if you have submitted a proof of concept project, you'll have done something that makes you a legitimate AI developer on the Azure platform. 
02:20 And that is something that you can take to your next job interview, to your next conversation about a promotion or a raise, and really have something significant to show. So Echo, I don't know why you'd be getting Echo. If anyone else is getting Echo, go ahead and put that into the chat. Maybe I can reconnect myself here. 
02:46 Let me double check my microphone setup. Everything looks correct. I've got echo cancellation turned on, noise suppression turned on. So keep an eye on the chat for more questions about echo. Sorry about if you're hearing any right now. First question for audience participation. Let's start off with an easy one. 
03:13 Where are you joining us from today? I'm in Seattle. Go ahead and put into the chat where you're tuning in from. I think it's always good to have a sense of where people are at in the world, make this community visible to each other. Thank you, Victor. No echo. I'm glad it's working for Victor. So, and I think, Victor, you're in Brazil, correct? Swati Gupta wants to know more about the hackathon. We'll get there very quickly. Okay. 
03:47 And Swati's coming in from India. Welcome. It's late at night for you there. Thank you for staying up with us. People tuning in from Mexico. We always get a good turnout from Mexico. My next question, what inspired you to sign up for the hackathon? What made you want to do this? Someone tuning in from Norway. Awesome. Welcome. Amsterdam. I'll continue to highlight where people are from if you put those in there. And if you've got anything you want to share about what inspired you to do this, was it because you just love building stuff? You wanted to geek out? Was it because you're looking to build your skills? 
04:47 You're looking to improve your job opportunities? Is it you've got an idea and you want to make it real? Those are all good reasons to do a hackathon. Wanted to learn more about AI. Yeah, the best way to do it is to use it. I'm surprised at how much I learn about AI every day. Because it's the innovation challenge right on. 
05:16 It is a strong community. This will be the sixth time that we've done it. And I've seen some amazing stuff come out of this community. And I've seen some amazing stuff come out of this community. If you could describe your ideal hackathon team in one word, what's one word that would describe that team you want to be on? Is it a fun team? Is it a smart team? Is it a talented team? Is it a fun team? Is it a smart team? Is it a talented team? Is it a resilient team? So with that, I'll go ahead and start to give some advice about the hackathon. 
06:15 So hold on a second while I share my screen. OK, here we are on the hackathon website. Throughout the hackathon, I'll continue to put more resources and other information into the challenges. But I didn't really get as much in there as I really wanted to. And so I figured I would sort of demonstrate how I'm using AI today. 
06:42 So here I am. This is the auto-olve Service Desk Challenge. I'm going to use Microsoft 365 Copilot. And let's just copy this in here. And let's start with, so for the following hackathon challenge, suggest what Azure services would be best to use. I've put in a prompt. And hey, welcome. Glad you made it here. Welcome from South America. 
07:33 Bienvenido. Should see more MVPs, especially during night coming up this evening, talking about hackathons regarding LATAM and BRICS opportunities on tech overseas and such. You know, what happens with Ignite is everyone's at Ignite and everyone's pretty busy. But I love the ask. More opportunities in Latin America as well as BRICS, Brazil, India and China, and so on. 
08:07 So, yeah, I think we should – it's always important for Microsoft to continue to – not off topic, don't worry. It's always important for Microsoft to continue to – that's why we do this. Opportunities for everyone in the world to – Hey, good to see you. The question is whether some challenges are more important than others when selecting the winners. 
08:29 You know, no, in this case, no. I think there's three of them this time, and they're all three pretty hard. Okay, so let's see what Copilot put in here. So for multi-agent service desk experience, best Azure services. So for the conversational interface, AI Foundry, OpenAI service, and Azure Bot Service plus Teams integration. 
09:01 Interesting. I might not do the Teams integration part of it. I've only got two weeks to make your proof of concept, but certainly AI Foundry and OpenAI Service are going to be key for a lot of projects. Knowledge retrieval, Azure AI Search, or Azure Cosmos DB or Azure SQL Database. So, yeah, you're going to need both search and a good database. 
09:30 Safe automation and runbook execution. The logic apps, for sure. I would definitely take a look at that. Transparency and trust. Application insights, Azure Monitor, Azure Key Vault, optional enhancements. I would actually, it's asking me if I want a visual diagram, but what I'm gonna ask it about, so transparency and trust, I actually would have expected it to say something about content safety. 
10:05 I'm sorry, I'm would have expected it to say something about content safety. So I'm actually going to put in here, in regard to transparency and trust, what are the best tools and processes to use to ensure that my project is in line with the Microsoft So I think that while all of the challenges are sort of equally weighted, again, remember that your project is going to be judged based on performance. 
11:04 Does it work? On innovation, is this something new? Have we seen it before? Are you using technology in a new way or changing the way that people are going to do things? Twenty-five percent on breadth of Azure. Are you using a few services or just one? How complicated is your project? Then 25 percent will be on responsible AI. 
11:27 And I think that, so here, let's see what Copilot has given us. Because they're definitely, so this is a little more what I was expecting for my first answer. So again, this is the key. It's what the standard is. This is the key. That's what the standard is. So AI content safety. That's what I would have expected. 
11:59 But the Azure AI Responsible AI Dashboard is part of Azure AI Studio. So bias protection, interoperability, and performance modeling. Now, you want to be sure that if an AI system, if people are putting information into an AI system, that the information they're getting back is what you would expect in terms of responsible AI, ethical AI. 
12:19 Is it sending back hate speech? That would be terrible. How do you prevent that? These are the tools and things you want to use to be sure. Interpret ML and FairLearn, part of Azure Machine Learning. Explain the fairness audits of ML components. And again, you might not have time to implement all of these, but if in your presentation you're talking through, this is what we would do when we take this to production, or when we build this beyond just a proof of concept, the judges are going to appreciate that. Again, the, you know, 
13:02 do a responsible AI impact assessment early on in, when you're building things through human in the loop escalation, clear thresholds for when automation hands off to a human. I think for this one, it's really important, especially if you're doing something like, you know, something that helps with a, you know, company's health insurance or something like that. 
13:23 And someone might have a real challenging health problem that they're getting back. You want to know when they should actually escalate it to a human who, the transparency, you know, why, you know, why am I saying this? Make sure that the AI system tells people why they're getting what they're getting. 
13:45 And then some great resources here. So create a checklist for hackathon participants that maps these to actionable steps. Yeah, let's go ahead and click that. Let's see what it does. This is pretty nice, actually. You know, go through each of the pillars and look at what it recommends as something you can actually do to show the judges that you've really thought about this, that you're really using the tools correctly. 
14:32 This is a nice one. Design for accessibility, for inclusiveness. Hackathon teams often include text-to-speech, speech-to-text, different sort of features that we often think of as accessibility features. The WCAG standards are a great resource for that. Then model cards, that's all good advice. I'm going to look at another one now. 
15:00 Customer personalization orchestration. This one is interesting. This is something that you'll actually see in a lot of Microsoft demos right now. We do a lot of things around Zava, an imaginary online retailer that is sort of a typical representative customer. And for this one, the idea is basically create a multi-agent solution that knows how to basically do marketing, create marketing messages that can be used as emails generally that get sent to potential customers based on what segment they're in. 
15:48 Segment being young person, male, female, if you want to do it like that, or works at large company, works at small company. I've tried to sort of find, oops, hit my back button. I think within here, you'll be able to find a lot of interesting data. But part of this challenge will probably be setting up who your segments are and figuring out what you want to do in terms of this. 
16:25 I'm going to go ahead and put in a new prompt for following hackathon challenge. Suggest the best Azure services to use. Just keep it really simple. It's nice. It gives you a good recommended stack. Best Azure services for each requirement. Multi-agent orchestration, AI Foundry, for sure. And actually, I'm kind of surprised it didn't come up with the Microsoft Agent Framework. 
17:15 I'm going to ask you a question about that. So we've got customer segmentation and data handling, Azure machine learning, content retrieval and compliance, AI search, safety and policy enforcement. This one's going to be really important. AI for content safety. If you're creating marketing messages based on who people are, you need to be sure that it's returning back appropriate messages to help them out. 
17:46 And again, the idea is you've got like a team of a marketing team out there. You're doing A-B testing. They want to send the best messages to get the best results, grow your business, but you have to do it right. And even hiring humans to do this is a hard, hard thing to do to get the humans to do it right. 
18:02 But if you're doing AI, it gets even harder. Experimentation and reporting, architecture flow. So how would the, I'm not even sure if I'm gonna call it the right thing, the Azure agent framework. I'm not even sure if I'm going to call it the right thing. The Azure Agent Framework be useful for this project. 
18:43 So, Azure Agent Framework, part of AI Foundry. Foundation for building multi-agent systems with orchestration, safety, and observability baked in. This is actually pretty helpful. This is helping me understand what the agent framework does. Multi-agent coordination. So it's actually sort of coming up with what the different agents are and making suggestions there just in terms of how you structure this. 
19:13 You want an agent to do each one of these different pieces. The agent framework is what's going to let these cooperate, share context, delegate tasks. The framework already integrates content safety with guardrails. You can define pre-send checks and escalation logic. Telemetry hooks, integration into Azure Modern Application Insights. 
19:48 Agents can call Azure services, make it easier to move from proof of concept production by leveraging Azure's governance compliance features, explainability, support structured reasoning traces. You can show why a message was generated, the content that was cited, and why a variant was blocked. Critical for transparency and trust. 
20:13 I think this really gets at something pretty valuable for a project. Explainability and the tools that we'd use to bake explainability into what your system is doing. I have to sample architecture diagram. I won to do that, provide a pseudo workflow snippet. Let's just go with what it's suggesting. Finding code like pseudocode. 
20:48 Nice. The compact pseudo workflow Python style shows how you can wire up a multi-agent system with the Azure agent framework for your challenge. It illustrates segmentation, content retrieval, citations, variant generation, safety checks, A, B, reblocking, escalation. It's not runnable code. But it's pretty interesting. 
21:13 I think this has given me a pretty good head start on what a team would need to do to get up and going and at least start playing around. It's using Fabric Lakehouse. You might want to use SQL Database. I think one thing I should probably be putting into this prompts is ask what can be done with the Azure free trial. 
21:36 I'll do that for the next one I'm about to do. It's got some pseudo code for the agent, retrieval agent. Video code for the agent, retrieval agent. Generate invariance grounded to retrieve content with inline citations. You are brand safe copywriter. Use brand tone. Cite sources like no PII, no medical financial claims, not, you know, no disallowed content. 
22:04 Safety agent, you're at least going for those. So that's pretty, I think that's, I'm actually learning a lot just by reading the pseudocode. So, and then it's saying what you're doing. So, hey, and Portia, welcome from Atlanta, Georgia. Appreciate you joining in. Recognize your name from email already. 
22:29 Okay, so I'm going to go ahead and go through the third challenge like this. Hopefully this is helpful for folks. So Civic Chat, this is the one that I discussed with Ken Granderson last week. You can find that on the site. Just to show you where you would find that if you missed it. Also show past activities. 
22:55 And then you can link over to that video that I did with Ken. Back over here. So for the following hackathon challenge, it's just the best Azure services that are part of the Azure free trial could be used So again, open AI service, bot service, AI search, Azure functions, very helpful because you'll be looking at public APIs, transforming civic data, very affordable. 
24:12 Azure app service, communication services, Azure translator, speech service. And let's see. Where can I find sample data to feed this system? We need to have the system understand how different political candidates, no it doesn't have to be candidates, different politicians would address a variety of issues. 
25:24 Pretty simple. Where can I find sample data to feed the system? You need to have the system understand how different politicians would address a variety of issues. And I'm putting this prompt in here because, again, looking for public data sets and sample data. This is coming up with way better stuff than I suggested on the site. 
25:45 This is coming up with way better stuff than I suggested on the site. The congressional record. Parsed speeches with metadata across decades supporting analysis of issue frequency and differences by party. That's interesting. Miller Center presidential speeches. Civic and ballot information APIs. Interesting. The U.S. Vote Foundation API. 
26:10 Ballot ready API. Google civic information API. There's a lot of information out there when you get into the government space. And suggested data strategy. So it's got the links in here. Conversational data strategy. So it's got the links in here, conversational data sets. So click here. 
26:38 What do I find? Rational record for the 43rd, 114th part speeches and abstract counts. This would be pretty interesting. match.consult. This would be pretty interesting. Come back here. So where could I find resources like this for both Brazil and Mexico. As a case here, not from the United States, might not be familiar with some of our political issues and problems. 
27:25 It's giving me all English language, it looks like. Maybe, maybe not. This looks good. Participa Brasil.gov Brazil. Okay. Governments make lots of information available. I'm almost at time. I'm going to stop sharing my screen. And, you know, again, I had to improvise a bit today because the person I wanted to introduce you to to talk about winning the Microsoft Hackathon, our global Microsoft Hackathon, wasn't able to join. 
27:57 He was really upset that he wasn't able to. He actually wanted to, like, reschedule, even though he's got some health issues. I was like, no, you go take care of your health. But that's why it was just me today. 
28:16 I hope that me showing you some ways to sort of work through with AI, how to frame up and look at the different challenges we put out there. Hopefully that's been helpful to folks. Feel free to put any questions or anything into the chat. I know that a lot of folks already have my email address. It's m-a-c-a-l-d-e at microsoft.com if you've got more questions. And I hope that this is a great experience for you, this hackathon. 
28:41 I hope you learn something. I hope you build confidence. And I hope you get inspired to build big stuff and take on big challenges. And I know that the Microsoft coaches and judges are all looking forward to seeing what you build because we know we'll be surprised. So thanks a ton. And keep an eye on the website where we keep adding events throughout the week, throughout the next two weeks and happy hacking. 
29:10 Take care.